{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8045357",
   "metadata": {},
   "source": [
    "# Self - RAG\n",
    "\n",
    "**Self-RAG**는 검색된 문서와 생성된 응답에 대한 자기 반선 (self-reflection) 및 자기 평가 (self-evaluation)를 포함한 RAG 전략으로, RAG 기반 시스템의 성능 향상에 기여할 수 있습니다. \n",
    "\n",
    "#### Self-RAG란?\n",
    "Self-RAG는 검색된 문서와 생성된 응답 모두에 대해 점검하고 검증하는 추가 단계를 포함하는 RAG 전략입니다. 전통적인 RAG 에서는 검색된 정보를 기반으로 LLM이 답변을 생성하는 것이 주된 과정이었다면, Self-RAG에서는 자체평가를 통해 다음과 같은 사항을 검증합니다. \n",
    "\n",
    "1. 검색할 필요성 판단 : 현재 질문에 대해 추가 검색이 필요한지 여부를 판단합니다. \n",
    "2. 검색 결과 관련성 평가 : 검색된 문서 조각(청크)이 질문 해결에 도움이 되는지 확인합니다. \n",
    "3. 응답 사실성 검증 : 생성된 답변이 제공된 문서 청크에 의해 충분히 뒷받침되는지 평가합니다. \n",
    "4. 응답 품질 평가 : 생성된 답변이 실제로 질문을 잘 해결하는지 측정합니다. \n",
    "\n",
    "[self-rag](https://arxiv.org/pdf/2310.11511)\n",
    "----------------------------------------\n",
    "#### Self-RAG 주요 개념 정리 \n",
    "1. **Retriever 사용 여부 결정** : 추가 검색을 진행할지, 검색없이 진행할지, 혹은 더 기다려볼것인지 결정합니다.\n",
    "* 입력 : `x (question)` 또는 `(x(question), y(generation))`\n",
    "* 출력 : `yes`, `no`, `continue`\n",
    "\n",
    "2. **관련성 평가 (Retrieval Grader)** : 검색된 문서 청크들이 실제로 질문을 받는데 유용한 정보인지 판별합니다. \n",
    "* 입력 : (x(question), d(chunk)) for each `d` in `D`\n",
    "* 출력 : `relevant` 또는 `irrelevant` \n",
    "\n",
    "3. **사실성 검증 (Hallucination Grader)**  : 생성된 응답이 검색 결과에 근거한 사실을 반영하는지, 혹은 환각 (hallucination)이 발생했는지 판단합니다. \n",
    "* 입력 : `x(question)`, `d(chunk)`, `y(generation)` for each `d` in `D`\n",
    "* 출력 : `{fully suppored, partially supported, no support}`\n",
    "\n",
    "3. **정답 품질 평가 (Answer Grader)** : 생성된 응답이 질문을 어느정도 해결하는지 점수화하여 평가합니다.\n",
    "* 입력 : `x(question)`, `y(generation)`\n",
    "* 출력 : `{5, 4, 3, 2, 1}`\n",
    "\n",
    "----------------------------------------\n",
    "본 내용에서는 LangGraph를 활용하여 Self-RAG 전략의 일부 아이디어를 구현하는 과정을 다룹니다.  \n",
    "다음과 같은 단계를 통해 Self-RAG전략을 구축하고 실행하는 방법을 익히게 됩니다.  \n",
    "\n",
    "* Retriever : 문서를 검색 \n",
    "* Retrieval Grader : 검색된 문서의 관련성 평가 \n",
    "* Generate : 질문에 대한 답변 생성 \n",
    "* Hallucination Grader : 생성된 답변의 사실성 (환각 여부) 검증\n",
    "* Answer Grader : 답변의 질문에 대한 관련성 평가 \n",
    "* Question Re-Writer : 쿼리 재작성 \n",
    "* 그래프 생성 및 실행 : 정의한 노드로 그래프를 빌드하고 실행 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a34ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from config.langsmith import langsmith \n",
    "\n",
    "langsmith(\"LangGraph-Self-RAG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e28c35",
   "metadata": {},
   "source": [
    "## PDF 기반 Retrieval Chain 생성 \n",
    "\n",
    "PDF 문서를 기반으로 Retrieval Chain을 생성합니다. 가장 단순한 구조의 Retrieval Chain입니다.  \n",
    "단, LangGraph에서는 Retriever와 Chain을 따로 생성합니다. 그래야 각 노드별로 세부 처리를 할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0482f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"문서 페이지 수 : {len(docs)}\")\n",
    "\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")\n",
    "\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = GoogleGenerativeAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "# FAISS (Facebook AI Similarity Search)는 밀집 벡터의 효율적인 유사도 검색과 클러스터링을 위한 라이브러리\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 검색기에 쿼리를 날려 검색된 chunk 결과를 확인합니다.\n",
    "# retriever.invoke(\"삼성전자가 자체 개발한 AI 의 이름은?\")\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Answer in Korean.\n",
    "\n",
    "    #Context: \n",
    "    {context}\n",
    "\n",
    "    #Question:\n",
    "    {question}\n",
    "\n",
    "    #Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatGoogleGenerativeAI(model_name=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 단계 9: 체인 실행(Run Chain)\n",
    "# 체인을 실행합니다.\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecdb1a",
   "metadata": {},
   "source": [
    "## 문서 검색 평가기 (Retrieval Grader)\n",
    "\n",
    "추후 retrieve 노드에서 문서에 대한 관련성 평가를 진행하기 위해 미리 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" A binary score to determine the relevance of the retrieved documents \"\"\"\n",
    "    binary_score :str = Field(\n",
    "        description = \"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.0-flash-001\",\n",
    "    temperature = 0,\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# 구조화된 출력 생성\n",
    "structred_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 시스템 프롬프트 정의 : 검색된 문서가 사용자 질문에 관련이 있는지 평가하는 시스템 역할 정의\n",
    "system_prompt = \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "    If the document contains keyworkds or semantic meaning related to the user question, grade it as relevant. \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\n",
    "\"\"\" \n",
    "\n",
    "# 채팅 프롬프트 템플릿 생성\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Retrieved document : \\n\\n {document} \\n\\n User question : {question}\")\n",
    "])\n",
    "\n",
    "# 검색 평가기 생성\n",
    "retrieval_grader = grade_prompt | structred_llm_grader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbf8bf",
   "metadata": {},
   "source": [
    "## 답변 생성 체인 \n",
    "\n",
    "Naive RAG 체인을 사용해서 검색된 문서를 기반으로 답변을 생성하는 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchian_google_genai import ChatGoogleGenerativeAI \n",
    "\n",
    "rag_prompt_system = \"\"\" \n",
    "    You are an AI assistant that can answer questions about the following context:\n",
    "    Your task is answering questions based on the context provided. \n",
    "    If you cannot find the answer in the context, just say \"I cannot find the answer in the context\".\n",
    "    You should answer in Korean. \n",
    "    Answer in English for name and technical terms. \n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_human = \"\"\" \n",
    "# question: {question}\n",
    "# context: {context}\n",
    "# answer: \n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_prompt_system),\n",
    "        (\"human\", rag_prompt_human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "rag_chain = rag_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "generation = rag_chain.invoke({\"context\" : format_docs(docs), \"question\" : question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bfd28",
   "metadata": {},
   "source": [
    "## 답변의 할루시네이션 여부 평가 \n",
    "`groundedness_grader` 를 생성하고 생성된 답변과 `context`를 기반하여 답변의 할루시네이션 평가를 진행합니다.  \n",
    "* `yes`인 경우 답변의 할루시네이션이 없음을 의미합니다.\n",
    "* `no`인 경우 답변이 할루시네이션이라고 간주합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83135592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class GroundednessGrader(BaseModel):\n",
    "    \"\"\" A binary score to determine the groundedness of the generated answer \"\"\"\n",
    "    binary_score :str = Field(\n",
    "        description = \"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.0-flash-001\",\n",
    "    temperature = 0,\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "groundedness_grader = llm.with_structured_output(GroundednessGrader)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n",
    "    Give a binary score 'yes' or 'no'. 'yes' means the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "\n",
    "groundedness_checking_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"set of facts : \\n\\n {documents} \\n\\n LLM generation : {generation}\")\n",
    "])\n",
    "\n",
    "# 답변의 할루시네이션 평가기 생성\n",
    "groundedness_grader = groundedness_checking_prompt | groundedness_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bb807",
   "metadata": {},
   "source": [
    "## 답변 관련성 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"A binary score indicating whether the question is addressed.\"\"\"\n",
    "\n",
    "    # 답변의 관련성 평가: 'yes' 또는 'no'로 표기(yes: 관련성 있음, no: 관련성 없음)\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.0-flash-001\",\n",
    "    temperature = 0,\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "# llm 에 GradeAnswer 바인딩\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# 프롬프트 생성\n",
    "answer_grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 답변 평가기 생성\n",
    "answer_grader = answer_grader_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567ebdf",
   "metadata": {},
   "source": [
    "## 질문 재작성기 (Question Rewriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 시스템 프롬프트 정의\n",
    "# 입력 질문을 벡터스토어 검색에 최적화된 형태로 변환하는 시스템 역할 정의\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# 시스템 메시지와 초기 질문을 포함한 프롬프트 템플릿 생성\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 질문 재작성기 생성\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250351d8",
   "metadata": {},
   "source": [
    "## 상태 정의\n",
    "\n",
    "* `question`: 사용자가 입력한 질문\n",
    "* `generation`: 생성된 응답\n",
    "* `documents`: 검색된 문서 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "# 그래프의 상태를 나타내는 클래스 정의\n",
    "class GraphState(TypedDict):\n",
    "    # 질문을 나타내는 문자열\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    # LLM에 의해 생성된 응답을 나타내는 문자열\n",
    "    generation: Annotated[str, \"LLM Generation\"]\n",
    "    # 문서의 목록을 나타내는 문자열 리스트\n",
    "    documents: Annotated[List[str], \"Retrieved Documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91257d1",
   "metadata": {},
   "source": [
    "## 노드 정의\n",
    "* retrieve: 문서 검색\n",
    "* grade_documents: 문서 평가\n",
    "* generate: 답변 생성\n",
    "* transform_query: 질문 재작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f642988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 검색\n",
    "def retrieve(state):\n",
    "    print(\"==== [RETRIEVE] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 검색 수행\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# 답변 생성\n",
    "def generate(state):\n",
    "    print(\"==== [GENERATE] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG 생성\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# 검색된 문서의 관련성 평가\n",
    "def grade_documents(state):\n",
    "    print(\"==== [GRADE DOCUMENTS] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 각 문서 점수 평가\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== GRADE: DOCUMENT RELEVANT ====\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"==== GRADE: DOCUMENT NOT RELEVANT ====\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "# 질문 변환\n",
    "def transform_query(state):\n",
    "    print(\"==== [TRANSFORM QUERY] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3b630",
   "metadata": {},
   "source": [
    "## 조건부 엣지 정의\n",
    "\n",
    "`decide_to_generate` 함수는 검색된 문서의 관련성 평가 결과에 따라 답변 생성 여부를 결정합니다.   \n",
    "\n",
    "`grade_generation_v_documents_and_question` 함수는 생성된 답변의 문서 및 질문과의 관련성 평가 결과에 따라 생성 여부를 결정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 생성 여부 결정\n",
    "def decide_to_generate(state):\n",
    "    print(\"==== [ASSESS GRADED DOCUMENTS] ====\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # 모든 문서가 관련성이 없는 경우\n",
    "        # 새로운 쿼리 생성\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # 관련 문서가 있는 경우 답변 생성\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# 생성된 답변의 문서 및 질문과의 관련성 평가\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"==== [CHECK HALLUCINATIONS] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = groundedness_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # 환각 여부 확인\n",
    "    if grade == \"yes\":\n",
    "        print(\"==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\")\n",
    "        # 질문 해결 여부 확인\n",
    "        print(\"==== [GRADE GENERATION vs QUESTION] ====\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [DECISION: GENERATION ADDRESSES QUESTION] ====\")\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            print(\"==== [DECISION: GENERATION DOES NOT ADDRESS QUESTION] ====\")\n",
    "            return \"not relevant\"\n",
    "    else:\n",
    "        print(\"==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====\")\n",
    "        return \"hallucination\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fefbd",
   "metadata": {},
   "source": [
    "## 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START \n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c512b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 문서 평가 노드에서 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "\n",
    "# 답변 생성 노드에서 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"hallucination\": \"generate\",\n",
    "        \"relevant\": END,\n",
    "        \"not relevant\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24061cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f062643",
   "metadata": {},
   "source": [
    "## 그래프 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"삼성전자가 개발한 생성형 AI 의 이름은?\",\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    app, inputs, config, [\"retrieve\", \"transform_query\", \"grade_documents\", \"generate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"테디노트가 개발한 생성형 AI 의 이름은?\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 그래프 실행\n",
    "    stream_graph(\n",
    "        app,\n",
    "        inputs,\n",
    "        config,\n",
    "        [\"retrieve\", \"transform_query\", \"grade_documents\", \"generate\"],\n",
    "    )\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
