{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRAG : Corrective RAG\n",
    "\n",
    "CRAG 전략을 사용하여 RAG 기반 시스템을 개선하는 방법을 다룹니다.  \n",
    "CRAG는 검색된 문서들에 자기에 대한 `자기반성 (self-reflection)` 및  `자기 평가 (self-evaluation)` 단계를 포함하여, 검색-생성 파이프라인을 정교하게 다루는 접금법 입니다  \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "#### CRAG란? \n",
    "**Corrective-RAG**는 RAG(Retrieval Augmented Generation) 전략에서 **검색 과정에서 찾아온 문서를 평가하고, 지식을 정제 (refine) 하는 단계를 추가한 방법론입니다. 이는 생성에 앞서 검색 결과를 점검하고 필요하다면 보조적인 검색을 수행하며, 최종적으로 품질 높은 답변을 생성하기 위한 일련의 프로세스를 포함합니다.\n",
    "\n",
    "CRAG의 핵심 아이디어는 다음과 같습니다.   \n",
    "\n",
    "[논문(Corrective Retrieval Augmented Generation)](https://arxiv.org/pdf/2401.15884)  \n",
    "1. 검색된 문서 중 하나 이상이 사전 정의된 관련성 임계값 (retrieval validation score) 을 초과하면 생성단계로 진행합니다.  \n",
    "2. 생성 전에 지식 정제 단계를 수행합니다.  \n",
    "3. 문서를 \"knowledge strips\" 로 세분화합니다. (여기서, 문서 검색 결과수, `k`를 의미합니다)\n",
    "4. 모든 문서가 관련성 임계값 이하이거나 평가 결과 신뢰도가 낮을 경우, 추가 데이터 소스 (예: 웹검색) 로 보강합니다. \n",
    "5. 모든 문서가 관련성 임계값 이하이거나 평가 결과 신뢰도가 낮을 경우, 추가 데이터 소스 (예: 웹검색)로 보강합니다. \n",
    "6. 웹 검색을 통한 보강 시, 쿼리 재작성 (Query-Rewrite) 을 통해 검색 결과를 최적화합니다. \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "#### 주요 내용\n",
    "LangGraph를 활용하여 CRAG 접근법의 일부 아이디어를 구현합니다.  \n",
    "여기서는 **지식 정제 단계는 생략** 하고, 필요하다면 노드로 추가할 수 있는 형태로 설계합니다.  \n",
    "또한, **관련 있는 문서가 하나도 없으면** 웹 검색을 활용하여 검색을 보완할 것입니다.\n",
    "웹 검색에는 **Tavily Search**를 사용하고, 검색 최적화를 위해 질문 재작성(Question Re-writing)을 도입합니다.  \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "#### 주요 \n",
    "* **Retrieval Grader** : 검색된 문서의 관련성을 평가  \n",
    "* **Generate** : LLM 을 통한 답변 생성  \n",
    "* **Question Re-writer** : 질문 재작성을 통한 검색 질의 최적화  \n",
    "* **Web Search Tool** : Tavily Search 를 통한 웹검색 활용 \n",
    "* **Create Graph** : LangGraph 를 통한 CRAG 전략 그래프 생성 \n",
    "* **Use the graph** : 생성된 그래프를 활용하는 방법 \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "#### 참고 \n",
    "* [LangGraph CRAG 튜토리얼 (공식 문서)](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag_local/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.16)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 환경설정 \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from config.langsmith import langsmith \n",
    "langsmith.set_project(\"langgraph-crag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 PDF 기반 Retrieval Chain 생성 \n",
    "\n",
    "PDF 문서를 기반으로 Retrieval Chain 을 생성합니다. 가장 단순한 구조의 Retrieval Chain입니다.   \n",
    "단, LangGraph에서는 Retriever와 Chain을 따로 생성합니다. 그래야 각 노드별로 세부 처리를 할 수 있습니다.  \n",
    "\n",
    "**실습에 활용한 문서** \n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "* 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "* 링크: https://spri.kr/posts/view/23669\n",
    "* 파일명: SPRI_AI_Brief_2023년12월호_F.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"문서 페이지 수 : {len(docs)}\")\n",
    "\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")\n",
    "\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = GoogleGenerativeAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "# FAISS (Facebook AI Similarity Search)는 밀집 벡터의 효율적인 유사도 검색과 클러스터링을 위한 라이브러리\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 검색기에 쿼리를 날려 검색된 chunk 결과를 확인합니다.\n",
    "# retriever.invoke(\"삼성전자가 자체 개발한 AI 의 이름은?\")\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Answer in Korean.\n",
    "\n",
    "    #Context: \n",
    "    {context}\n",
    "\n",
    "    #Question:\n",
    "    {question}\n",
    "\n",
    "    #Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatGoogleGenerativeAI(model_name=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 단계 9: 체인 실행(Run Chain)\n",
    "# 체인을 실행합니다.\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 문서의 관련성 평가 (Question-Retrieval Evaluation)\n",
    "\n",
    "검색된 문서의 관련성 평가는 검색된 문서가 질문과 관련이 있는지 여부를 평가하는 단계입니다.  \n",
    "먼저, 검색된 문서를 평가하기 위한 `평가기(retrieval-grader)` 를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field \n",
    "\n",
    "# 검색된 문서의 관련성 여부를 이진 점수로 평가하는 데이터 모델 \n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Binary score to determine the relevance of the retrieved document\"\"\"\n",
    "\n",
    "    # 문서가 질문과 관련이 있는지 여부를 'yes' 또는 'no' 로 나타내는 필드 \n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# GradeDocuments 데이터 모델을 사용하여 구조화된 출력을 생성 \n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system = \"\"\" \n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords or semantic meaning related to the question, grade it as relevant.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "# 채팅 프롬프트 탬플릿 생성\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieval 평가기 초기화\n",
    "retrieval_grader = grade_prompt | structured_llm_grader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retrieval_grader`를 사용해서 문서를 평가합니다.   \n",
    "여기서는 문서의 집합이 아닌 1개의 단일 문서에 대한 평가를 수행합니다.  \n",
    "결과는 단일 문서에 대한 관련성 여부가 (yes / no) 로 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 정의 \n",
    "question = \"삼성전자가 개발한 생성AI에 대해 설명하세요\"\n",
    "\n",
    "# 문서 검색 \n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "# 문서 평가 \n",
    "# 검색된 문서 중 0번째 index 문서의 페이지 내용 추출 \n",
    "doc_text = docs[0].page_content \n",
    "\n",
    "# 검색된 문서와 질문을 사용해서 관련성 평가를 실행하고 결과 출력 \n",
    "print(retrieval_grader.invoke({\"question\":question, \"document\":doc_text}))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답변 생성 체인 \n",
    "\n",
    "답변 생성 체인은 검색된 문서를 기반으로 답변을 생성하는 체인입니다.   \n",
    "우리가 알고 있는 일반적인 Naive RAG 체인입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchian_google_genai import ChatGoogleGenerativeAI \n",
    "\n",
    "rag_prompt_system = \"\"\" \n",
    " You are an AI assistant that can answer questions about the following context:\n",
    " Your task is answering questions based on the context provided. \n",
    " If you cannot find the answer in the context, just say \"I cannot find the answer in the context\".\n",
    " You should answer in Korean. \n",
    " Answer in English for name and technical terms. \n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_human = \"\"\" \n",
    "# question: {question}\n",
    "# context: {context}\n",
    "# answer: \n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_prompt_system),\n",
    "        (\"human\", rag_prompt_human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "rag_chain = rag_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "generation = rag_chain.invoke({\"context\" : format_docs(docs), \"question\" : question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 쿼리 재작성 (Question Re-writer)\n",
    "\n",
    "쿼리 재작성은 웹 검색을 최적화하기 위해 질문을 재작성하는 단계입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 재작성 (Question Re-writer) 시스템 프롬프트\n",
    "system_prompt = \"\"\" \n",
    "    You are a question re-writer that converts an input question to a better version that is optimized for web search. \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 정의\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Question Re-writer 체인 초기화 \n",
    "re_write_chain = re_write_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(f\"[원본 질문] : {question}\")\n",
    "print(f\"[재작성 질문] :\", re_write_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹 검색 도구 \n",
    "**웹 검색 도구**는 컨텍스트를 보강하기 위한 용도로 사용됩니다. \n",
    "* **웹 검색의 필요성** : 모든 문서가 관련성 임계값을 충족하지 않거나 평가자가 확신이 없을 때, 웹 검색을 통해 추가 데이터를 확보합니다. \n",
    "* **Tavily Search 사용** : Tavily Search를 활용하여 웹검색을 수행합니다. 이는 검색 쿼리를 최적화 하고, 보다 관련성 높은 결과를 제공합니다. \n",
    "* **질문 재작성** : 웹 검색을 최적화하기 위해 질문을 재작성하여 검색 쿼리를 개선합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.16)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 웹 검색 도구 초기화\n",
    "from config.tavily_search import TavilySearch \n",
    "\n",
    "# 최대 검색 결과를 3으로 설정\n",
    "web_search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "results = web_search_tool.invoke({\"query\":question})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태 (State)\n",
    "\n",
    "CRAG 그래프를 위한 상태를 정의합니다.   \n",
    "`web_search`는 웹 검색을 활용할지 여부를 나타내는 상태입니다.  \n",
    "**yes** 또는 **no** 로 표현합니다 (yes : 웹 검색 필요, no : 필요없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List \n",
    "from typing_extensions import TypedDict \n",
    "\n",
    "# 상태정의 \n",
    "class GraphState(TypedDict):\n",
    "    question : Annotated[str, \"The question to answer\"]\n",
    "    generation : Annotated[str, \"The generation from the LLM\"]\n",
    "    web_search : Annotated[str, \"whether to add search\"]\n",
    "    documents: Annotated[List[str], \"The documents retrieved from the vector store\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노드 \n",
    "\n",
    "CRAG 그래프에 활용할 노드를 정의합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document \n",
    "\n",
    "# 문서 검색 노드\n",
    "def retrieve(state: GraphState):\n",
    "    print(\"===Retrieve===\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 문서 검색 \n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "# 답변 생성 노드\n",
    "def generate(state: GraphState):\n",
    "    print(\"===Generate===\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG를 사용한 답변 생성 \n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\":question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# 문서 평가 노드 \n",
    "def grade_documents(state: GraphState):\n",
    "    print(\"===Grade Documents===\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 필터링된 문서 \n",
    "    filtered_docs = [] \n",
    "    relevant_doc_count = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        # Question - Document 관련성 평가\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": doc.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade == \"yes\":\n",
    "            print(\"=== [GRADE : DOCUMENT RELEVANT] ===\")\n",
    "            # 관련 있는 문서를 filtered_docs에 추가 \n",
    "            filtered_docs.append(doc)\n",
    "            relevant_doc_count += 1\n",
    "        else: \n",
    "            print(\"=== [GRADE : DOCUMENT IRRELEVANT] ===\")\n",
    "            continue \n",
    "\n",
    "    # 관련 문서가 없으면 웹 검색 수행\n",
    "    web_search = \"yes\" if relevant_doc_count == 0 else \"no\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "\n",
    "# 쿼리 재작성 노드\n",
    "def query_rewrite(state: GraphState):\n",
    "    print(\"===Query Rewrite===\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = re_write_chain.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "# 웹 검색 노드\n",
    "def web_search(state: GraphState):\n",
    "    print(\"===Web Search===\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 웹 검색 실행\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    # 검색 결과를 문서 형식으로 변환 \n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    \n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조건부 엣지에 활용할 함수 \n",
    "`decide_to_generate` 함수는 관련성 평가를 마친 뒤, 웹 검색 여부에 따라 다음 노드로 라우팅하는 역할을 수행합니다.    \n",
    "`web_search`가 `YES`인 경우 `query_rewirte` 노드에서 쿼리를 재작성 한 뒤 웹 검색을 수행합니다.   \n",
    "만약, `web_search`가 `No`인 경우는 `generate`를 수행하여 최종 답변을 생성합니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    # 평가된 문서를 기반으로 다음 단계를 결정\n",
    "    print(\"===Decide to Generate===\")\n",
    "    # 웹 검색 필요 여부\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"yes\":\n",
    "        # 웹 검색으로 정보 보강이 필요한 경우\n",
    "        print(\"=== all documents are irrelevant ===\")\n",
    "        return \"query_rewrite\"\n",
    "    else:\n",
    "        # 관련 문서가 존재하므로 답변 생성 단계(generate)로 진행\n",
    "        print(\"=== some documents are relevant ===\")\n",
    "        return \"generate\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 생성\n",
    "\n",
    "노드를 정의하고 엣지를 연결하여 그래프를 완성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# 그래프 상태 초기화 \n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generage\", generate)\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "# 엣지 연결 \n",
    "workflow.add_edge(START,\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 문서 평가 노드에서 조건부 엣지 추가 \n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"query_rewrite\" : \"query_rewrite\",\n",
    "        \"generate\" : \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.add_edge(\"query_rewrite\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchian_core.runnables import RunnableConfig \n",
    "from config.messages import stream_graph, invoke_graph, random_uuid \n",
    "\n",
    "# config 설정 (재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력 \n",
    "inputs = {\"question\": \"삼성전자가 개발한 생성AI에 대해 설명하세요\"}\n",
    "\n",
    "# 스트리밍 형식으로 그래프 실행 \n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs, \n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"generate\", \"query_rewrite\", \"web_search\", \"generate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정 (재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\":random_uuid()}) \n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\"question\" : \"2024년 노벨 문학상 수상자의 이름은?\"}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"query_rewrite\", \"generate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
